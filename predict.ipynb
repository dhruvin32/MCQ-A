{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2lGQIIpwJxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncdFsDJPv0ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "import glob \n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertForMultipleChoice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvE_IuB3x-PP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "766fd20c-b428-4913-cfa8-0071f44de07d"
      },
      "source": [
        "basePath = '/content/'\n",
        "\n",
        "data_dir = basePath+'dataset'\n",
        "bert_model = 'bert-large-uncased'\n",
        "drive_dir = basePath+'drive/My Drive/temp/Dataset'\n",
        "output_model_file = drive_dir+'/pytorch_model.bin'\n",
        "\n",
        "max_seq_length = 320\n",
        "do_eval = True\n",
        "do_lower_case = True\n",
        "eval_batch_size = 1\n",
        "no_cuda = False  # False = use cuda if available\n",
        "local_rank = -1\n",
        "seed = 42"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gradient_accumulation_steps = 2\\nfp16 = True\\nloss_scale = 128'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DVH8M2tzxgV",
        "colab_type": "code",
        "outputId": "c7d24fa5-20b6-4b89-f7db-46eb24c5c2f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''import zipfile\n",
        "import os\n",
        "for file_name in os.listdir(drive_dir):\n",
        "  if file_name.endswith('.zip'):\n",
        "    with zipfile.ZipFile(drive_dir+'/'+file_name,'r') as zip_dir:\n",
        "      zip_dir.extractall(path='/content/')'''"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import zipfile\\nimport os\\nfor file_name in os.listdir(drive_dir):\\n  if file_name.endswith('.zip'):\\n    with zipfile.ZipFile(drive_dir+'/'+file_name,'r') as zip_dir:\\n      zip_dir.extractall(path='/content/')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlcKyn1TwBQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCqskyLcwElO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RaceExample(object):\n",
        "    \"\"\"A single training/test example for the RACE dataset.\"\"\"\n",
        "    '''\n",
        "    For RACE dataset:\n",
        "    race_id: data id\n",
        "    context_sentence: article\n",
        "    start_ending: question\n",
        "    ending_0/1/2/3: option_0/1/2/3\n",
        "    label: true answer\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 race_id,\n",
        "                 context_sentence,\n",
        "                 start_ending,\n",
        "                 ending_0,\n",
        "                 ending_1,\n",
        "                 ending_2,\n",
        "                 ending_3,\n",
        "                 label = None):\n",
        "        self.race_id = race_id\n",
        "        self.context_sentence = context_sentence\n",
        "        self.start_ending = start_ending\n",
        "        self.endings = [\n",
        "            ending_0,\n",
        "            ending_1,\n",
        "            ending_2,\n",
        "            ending_3,\n",
        "        ]\n",
        "        self.label = label\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        l = [\n",
        "            f\"id: {self.race_id}\",\n",
        "            f\"article: {self.context_sentence}\",\n",
        "            f\"question: {self.start_ending}\",\n",
        "            f\"option_0: {self.endings[0]}\",\n",
        "            f\"option_1: {self.endings[1]}\",\n",
        "            f\"option_2: {self.endings[2]}\",\n",
        "            f\"option_3: {self.endings[3]}\",\n",
        "        ]\n",
        "\n",
        "        if self.label is not None:\n",
        "            l.append(f\"label: {self.label}\")\n",
        "\n",
        "        return \", \".join(l)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anP7lPY6xkp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    def __init__(self,\n",
        "                 example_id,\n",
        "                 choices_features,\n",
        "                 label\n",
        "\n",
        "    ):\n",
        "        self.example_id = example_id\n",
        "        self.choices_features = [\n",
        "            {\n",
        "                'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'segment_ids': segment_ids\n",
        "            }\n",
        "            for _, input_ids, input_mask, segment_ids in choices_features\n",
        "        ]\n",
        "        self.label = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpmlldL_xnh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## paths is a list containing all paths\n",
        "def read_race_examples(paths):\n",
        "    examples = []\n",
        "    for path in paths:\n",
        "        filenames = glob.glob(path+\"/*json\")\n",
        "        for filename in filenames:\n",
        "            with open(filename, 'r', encoding='utf-8') as fpr:\n",
        "                data_raw = json.load(fpr)\n",
        "                article = data_raw['article']\n",
        "                ## for each qn\n",
        "                for i in range(len(data_raw['answers'])):\n",
        "                    truth = ord(data_raw['answers'][i]) - ord('A')\n",
        "                    question = data_raw['questions'][i]\n",
        "                    options = data_raw['options'][i]\n",
        "                    examples.append(\n",
        "                        RaceExample(\n",
        "                            race_id = filename+'-'+str(i),\n",
        "                            context_sentence = article,\n",
        "                            start_ending = question,\n",
        "\n",
        "                            ending_0 = options[0],\n",
        "                            ending_1 = options[1],\n",
        "                            ending_2 = options[2],\n",
        "                            ending_3 = options[3],\n",
        "                            label = truth))\n",
        "                \n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWHk7eBxsog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
        "                                 is_training):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    # RACE is a multiple choice task. To perform this task using Bert,\n",
        "    # we will use the formatting proposed in \"Improving Language\n",
        "    # Understanding by Generative Pre-Training\" and suggested by\n",
        "    # @jacobdevlin-google in this issue\n",
        "    # https://github.com/google-research/bert/issues/38.\n",
        "    #\n",
        "    # The input will be like:\n",
        "    # [CLS] Article [SEP] Question + Option [SEP]\n",
        "    # for each option \n",
        "    # \n",
        "    # The model will output a single value for each input. To get the\n",
        "    # final decision of the model, we will run a softmax over these 4\n",
        "    # outputs.\n",
        "    features = []\n",
        "    for example_index, example in enumerate(examples):\n",
        "        context_tokens = tokenizer.tokenize(example.context_sentence)\n",
        "        start_ending_tokens = tokenizer.tokenize(example.start_ending)\n",
        "\n",
        "        choices_features = []\n",
        "        for ending_index, ending in enumerate(example.endings):\n",
        "            # We create a copy of the context tokens in order to be\n",
        "            # able to shrink it according to ending_tokens\n",
        "            context_tokens_choice = context_tokens[:]\n",
        "            ending_tokens = start_ending_tokens + tokenizer.tokenize(ending)\n",
        "            # Modifies `context_tokens_choice` and `ending_tokens` in\n",
        "            # place so that the total length is less than the\n",
        "            # specified length.  Account for [CLS], [SEP], [SEP] with\n",
        "            # \"- 3\"\n",
        "            _truncate_seq_pair(context_tokens_choice, ending_tokens, max_seq_length - 3)\n",
        "\n",
        "            tokens = [\"[CLS]\"] + context_tokens_choice + [\"[SEP]\"] + ending_tokens + [\"[SEP]\"]\n",
        "            segment_ids = [0] * (len(context_tokens_choice) + 2) + [1] * (len(ending_tokens) + 1)\n",
        "\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "            input_mask = [1] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding = [0] * (max_seq_length - len(input_ids))\n",
        "            input_ids += padding\n",
        "            input_mask += padding\n",
        "            segment_ids += padding\n",
        "\n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segment_ids) == max_seq_length\n",
        "\n",
        "            choices_features.append((tokens, input_ids, input_mask, segment_ids))\n",
        "\n",
        "        label = example.label\n",
        "        ## display some example\n",
        "        if example_index < 1:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(f\"race_id: {example.race_id}\")\n",
        "            for choice_idx, (tokens, input_ids, input_mask, segment_ids) in enumerate(choices_features):\n",
        "                logger.info(f\"choice: {choice_idx}\")\n",
        "                logger.info(f\"tokens: {' '.join(tokens)}\")\n",
        "                logger.info(f\"input_ids: {' '.join(map(str, input_ids))}\")\n",
        "                logger.info(f\"input_mask: {' '.join(map(str, input_mask))}\")\n",
        "                logger.info(f\"segment_ids: {' '.join(map(str, segment_ids))}\")\n",
        "            if is_training:\n",
        "                logger.info(f\"label: {label}\")\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                example_id = example.race_id,\n",
        "                choices_features = choices_features,\n",
        "                label = label\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwcBWSyAxulF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T16BaD2Yxwge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(out, labels):\n",
        "    outputs = np.argmax(out, axis=1)\n",
        "    return np.sum(outputs == labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB_sayHExyF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def select_field(features, field):\n",
        "    return [\n",
        "        [\n",
        "            choice[field]\n",
        "            for choice in feature.choices_features\n",
        "        ]\n",
        "        for feature in features\n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Sr4oXOxzrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0 - x'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9jTTz1zx12f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "\n",
        "    if local_rank == -1 or no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "    else:\n",
        "        torch.cuda.set_device(local_rank)\n",
        "        device = torch.device(\"cuda\", local_rank)\n",
        "        n_gpu = 1\n",
        "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n",
        "\n",
        "\n",
        "    if n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Load a trained model that you have fine-tuned\n",
        "    # use this part if you want to load the trained model\n",
        "    model_state_dict = torch.load(output_model_file)\n",
        "    model = BertForMultipleChoice.from_pretrained(bert_model,\n",
        "         state_dict=model_state_dict,\n",
        "         num_choices=4)\n",
        "    model.to(device)\n",
        "\n",
        "    if do_eval and (local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "        test_dir = os.path.join(data_dir, 'test')\n",
        "        test_high = [test_dir ]\n",
        "\n",
        "        eval_examples = read_race_examples(test_high)\n",
        "        eval_features = convert_examples_to_features(\n",
        "            eval_examples, tokenizer, max_seq_length, True)\n",
        "        logger.info(\"***** Running evaluation: test *****\")\n",
        "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
        "        logger.info(\"  Batch size = %d\", eval_batch_size)\n",
        "        all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long)\n",
        "        all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long)\n",
        "        all_label = torch.tensor([f.label for f in eval_features], dtype=torch.long)\n",
        "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
        "        # Run prediction for full data\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
        "\n",
        "        model.eval()\n",
        "        high_eval_loss, high_eval_accuracy = 0, 0\n",
        "        high_nb_eval_steps, high_nb_eval_examples = 0, 0\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, input_mask, segment_ids, label_ids = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "                logits = model(input_ids, segment_ids, input_mask)\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = label_ids.to('cpu').numpy()\n",
        "\n",
        "            \n",
        "            question = eval_examples[step].start_ending\n",
        "            options = 'A. '+eval_examples[step].endings[0]+'    B. '+eval_examples[step].endings[1]+'    c. '+eval_examples[step].endings[2]+'    D. '+eval_examples[step].endings[3]\n",
        "            \n",
        "            ansOpt = chr( np.argmax(logits[0]) + ord('A') )\n",
        "            answer = ansOpt+'. '+eval_examples[step].endings[np.argmax(logits[0])]\n",
        "            \n",
        "            print(question)\n",
        "            print(options)\n",
        "            print(answer)\n",
        "            #print(label_ids)            \n",
        "                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIBYPiqJzrin",
        "colab_type": "code",
        "outputId": "62b07939-ada3-48ff-fab8-dd3b6e6cd858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/20/2020 19:18:13 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/20/2020 19:18:26 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz not found in cache, downloading to /tmp/tmp244qf2pc\n",
            "100%|██████████| 1248501532/1248501532 [00:42<00:00, 29252312.99B/s]\n",
            "04/20/2020 19:19:09 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmp244qf2pc to cache at /root/.pytorch_pretrained_bert/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05\n",
            "04/20/2020 19:19:13 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05\n",
            "04/20/2020 19:19:13 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmp244qf2pc\n",
            "04/20/2020 19:19:13 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05\n",
            "04/20/2020 19:19:13 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05 to temp dir /tmp/tmpoi0du887\n",
            "04/20/2020 19:19:48 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   *** Example ***\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   race_id: /content/dataset/test/traindata2.json-0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   choice: 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   tokens: [CLS] without co ##rio ##lis effect the global winds would blow north to south or south to north . but co ##rio ##lis makes them blow northeast to southwest or the reverse in the northern hemisphere . the winds blow northwest to southeast or the reverse in the southern hemisphere . [SEP] what phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere ? mu ##on effect [SEP]\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   input_ids: 101 2302 2522 9488 6856 3466 1996 3795 7266 2052 6271 2167 2000 2148 2030 2148 2000 2167 1012 2021 2522 9488 6856 3084 2068 6271 4794 2000 4943 2030 1996 7901 1999 1996 2642 14130 1012 1996 7266 6271 4514 2000 4643 2030 1996 7901 1999 1996 2670 14130 1012 102 2054 9575 3084 3795 7266 6271 4794 2000 4943 2030 1996 7901 1999 1996 2642 14130 1998 4514 2000 4643 2030 1996 7901 1999 1996 2670 14130 1029 14163 2239 3466 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   choice: 1\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   tokens: [CLS] without co ##rio ##lis effect the global winds would blow north to south or south to north . but co ##rio ##lis makes them blow northeast to southwest or the reverse in the northern hemisphere . the winds blow northwest to southeast or the reverse in the southern hemisphere . [SEP] what phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere ? cent ##ri ##fu ##gal effect [SEP]\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   input_ids: 101 2302 2522 9488 6856 3466 1996 3795 7266 2052 6271 2167 2000 2148 2030 2148 2000 2167 1012 2021 2522 9488 6856 3084 2068 6271 4794 2000 4943 2030 1996 7901 1999 1996 2642 14130 1012 1996 7266 6271 4514 2000 4643 2030 1996 7901 1999 1996 2670 14130 1012 102 2054 9575 3084 3795 7266 6271 4794 2000 4943 2030 1996 7901 1999 1996 2642 14130 1998 4514 2000 4643 2030 1996 7901 1999 1996 2670 14130 1029 9358 3089 11263 9692 3466 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   choice: 2\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   tokens: [CLS] without co ##rio ##lis effect the global winds would blow north to south or south to north . but co ##rio ##lis makes them blow northeast to southwest or the reverse in the northern hemisphere . the winds blow northwest to southeast or the reverse in the southern hemisphere . [SEP] what phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere ? tropical effect [SEP]\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   input_ids: 101 2302 2522 9488 6856 3466 1996 3795 7266 2052 6271 2167 2000 2148 2030 2148 2000 2167 1012 2021 2522 9488 6856 3084 2068 6271 4794 2000 4943 2030 1996 7901 1999 1996 2642 14130 1012 1996 7266 6271 4514 2000 4643 2030 1996 7901 1999 1996 2670 14130 1012 102 2054 9575 3084 3795 7266 6271 4794 2000 4943 2030 1996 7901 1999 1996 2642 14130 1998 4514 2000 4643 2030 1996 7901 1999 1996 2670 14130 1029 5133 3466 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   choice: 3\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   tokens: [CLS] without co ##rio ##lis effect the global winds would blow north to south or south to north . but co ##rio ##lis makes them blow northeast to southwest or the reverse in the northern hemisphere . the winds blow northwest to southeast or the reverse in the southern hemisphere . [SEP] what phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere ? co ##rio ##lis effect [SEP]\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   input_ids: 101 2302 2522 9488 6856 3466 1996 3795 7266 2052 6271 2167 2000 2148 2030 2148 2000 2167 1012 2021 2522 9488 6856 3084 2068 6271 4794 2000 4943 2030 1996 7901 1999 1996 2642 14130 1012 1996 7266 6271 4514 2000 4643 2030 1996 7901 1999 1996 2670 14130 1012 102 2054 9575 3084 3795 7266 6271 4794 2000 4943 2030 1996 7901 1999 1996 2642 14130 1998 4514 2000 4643 2030 1996 7901 1999 1996 2670 14130 1029 2522 9488 6856 3466 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   label: 3\n",
            "04/20/2020 19:19:59 - INFO - __main__ -   ***** Running evaluation: test high *****\n",
            "04/20/2020 19:19:59 - INFO - __main__ -     Num examples = 3\n",
            "04/20/2020 19:19:59 - INFO - __main__ -     Batch size = 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?\n",
            "A. muon effect    B. centrifugal effect    c. tropical effect    D. coriolis effect\n",
            "B. centrifugal effect\n",
            "What type of organism is commonly used in preparation of foods such as cheese and yogurt?\n",
            "A. mesophilic organisms    B. gymnosperms    c. viruses    D. protozoa\n",
            "A. mesophilic organisms\n",
            "Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always what?\n",
            "A. unbalanced    B. reactive    c. endothermic    D. exothermic\n",
            "D. exothermic\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}