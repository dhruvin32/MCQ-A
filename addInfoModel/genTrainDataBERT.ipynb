{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "genDataBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-xfch3Wm-_t",
        "colab_type": "code",
        "outputId": "80b404b7-b4b8-46b9-f31f-570beb82e6d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''!pip install tensorflow-gpu==2.0\n",
        "!pip install tensorflow_hub\n",
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!pip install tensorflow-gpu==2.0\\n!pip install tensorflow_hub\\n!pip install bert-for-tf2\\n!pip install sentencepiece'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPTceEMHnHwB",
        "colab_type": "code",
        "outputId": "c4c1c2a7-642f-4e8b-fe41-5d1aac7deafe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TF version: \", tf.__version__)\n",
        "print(\"Hub version: \", hub.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version:  2.0.0\n",
            "Hub version:  0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvtkzvXvnRED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
        "#import math\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejcPhDeHP106",
        "colab_type": "code",
        "outputId": "e4089d71-f95e-4f75-bf76-ad492eaaf4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOmwpE9-nUmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "basePath = '/content/'\n",
        "\n",
        "data_dir = basePath+'dataset'\n",
        "drive_dir = basePath+'drive/My Drive/temp/DatasetN'\n",
        "paraFile = basePath + 'drive/My Drive/nlpproject/paragraph2.json'\n",
        "\n",
        "'''data_dir = basePath+'tempdataset'\n",
        "drive_dir = basePath+'drive/My Drive/temp/tempDataset'\n",
        "paraFile = basePath + 'drive/My Drive/temp.json' '''\n",
        "\n",
        "outDir = basePath+'test'\n",
        "\n",
        "max_seq_length = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIN9aaUtneTt",
        "colab_type": "code",
        "outputId": "05d8be41-cfbf-4403-cb0f-f76fcdd83839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''for file_name in os.listdir(drive_dir):\n",
        "    if file_name.endswith('.zip'):\n",
        "        with zipfile.ZipFile(drive_dir+'/'+file_name,'r') as zip_dir:\n",
        "            zip_dir.extractall(path='/content/')'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"for file_name in os.listdir(drive_dir):\\n    if file_name.endswith('.zip'):\\n        with zipfile.ZipFile(drive_dir+'/'+file_name,'r') as zip_dir:\\n            zip_dir.extractall(path='/content/')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4UVAOvYnlM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getBERTModel():\n",
        "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,name=\"input_mask\")\n",
        "    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,name=\"segment_ids\")\n",
        "\n",
        "    bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=False)\n",
        "    #bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\",trainable=False)\n",
        "    \n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "\n",
        "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])\n",
        "\n",
        "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "    FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "    tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "    return {'model':model,'tokenizer':tokenizer}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcB1jlg3npsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_masks(tokens, max_seq_length):\n",
        "    #print('len(tokens),max_seq_length)\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_ids(tokens, tokenizer, max_seq_length):\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqx9t0z9nrxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getEmbeddings(model,tokenizer,sentence): \n",
        "    stokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "    if len(stokens) > (max_seq_length - 2):\n",
        "      stokens = stokens[:max_seq_length-2]\n",
        "\n",
        "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        "\n",
        "    input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
        "    #print(len(input_ids))\n",
        "    input_masks = get_masks(stokens, max_seq_length)\n",
        "    input_segments = get_segments(stokens, max_seq_length)\n",
        "\n",
        "    '''print(input_masks)\n",
        "    print(input_segments)'''\n",
        "\n",
        "    pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
        "    '''print('see')\n",
        "    print(all_embs.shape)\n",
        "    print(pool_embs.shape)'''\n",
        "    # pool_ebmbs is an embeding of CLS token\n",
        "    # all_embs contains embeding for words of input sentence.\n",
        "    return pool_embs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zYNsbW5nuXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BERTEmbeddings(model,tokenizer,nSentences):\n",
        "    trainX = np.asarray([])\n",
        "    n = len(nSentences)\n",
        "    for i in range(n): \n",
        "        if i%500 == 0:\n",
        "          print( 'Processing ',i,' out of ',n)\n",
        "\n",
        "        #senLen = len(nSentences[i].split())\n",
        "        embs = getEmbeddings(model,tokenizer,nSentences[i])\n",
        "        '''print(embs.shape)\n",
        "        x = embs'''\n",
        "        if trainX.shape[0] == 0:\n",
        "            trainX = embs\n",
        "        else:\n",
        "            trainX = np.concatenate((trainX, embs), axis=0)\n",
        "    return trainX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRGEkkRhnwdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getFileSent(inpFile):\n",
        "    fileSent = ''\n",
        "    article = ''\n",
        "    with open(inpFile,'r') as f:\n",
        "        x = json.loads(f.read())\n",
        "        article = x['article']\n",
        "        for option in x['options'][0]:\n",
        "            fileSent += option\n",
        "            fileSent += ' '\n",
        "        fileSent += x['questions'][0]\n",
        "    return fileSent,article"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiO2Gv6wny3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def saveFile(inpFile,outFile,para):\n",
        "    with open(inpFile,'r') as f:\n",
        "      x = json.loads(f.read())\n",
        "      x['article'] = para\n",
        "\n",
        "      with open(outFile, 'w') as jsonOut:\n",
        "          json.dump(x, jsonOut)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcqXlxfZn4ew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outModel = getBERTModel()\n",
        "model = outModel['model']\n",
        "tokenizer = outModel['tokenizer']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHYXV8xan8Jl",
        "colab_type": "code",
        "outputId": "81fd8944-d593-4d1a-a170-2819bb6b75a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''f = open(paraFile,'r')\n",
        "x = json.loads(f.read())\n",
        "f.close()\n",
        "fileSents = [ x['para'+str(id)] for id in range(1,len(x)+1)]'''\n",
        "#print(len(fileSents))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"f = open(paraFile,'r')\\nx = json.loads(f.read())\\nf.close()\\nfileSents = [ x['para'+str(id)] for id in range(1,len(x)+1)]\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLbmjYwfn-3e",
        "colab_type": "code",
        "outputId": "ae10cdef-4efa-454b-f314-a0d6485f3553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''paraEembs = BERTEmbeddings(model,tokenizer,fileSents)\n",
        "print(paraEembs.shape)'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'paraEembs = BERTEmbeddings(model,tokenizer,fileSents)\\nprint(paraEembs.shape)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSPj0fhmn_vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#np.savetxt(basePath+'paraEmbs.csv',paraEembs, delimiter=',',fmt='%.15f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzcfZrxwoDDd",
        "colab_type": "code",
        "outputId": "1290ee7d-c3c1-4995-c45d-6c6d9e204a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "#totalfile = sum([len(files) for r, d, files in os.walk(\"/content/dataset/test/\")])\n",
        "totalfile = sum([len(files) for r, d, files in os.walk(data_dir+\"/train/\")])\n",
        "#totalfile = sum([len(files) for r, d, files in os.walk(\"/content/dataset/dev/\")])\n",
        "\n",
        "\n",
        "print(totalfile)\n",
        "count = 0\n",
        "setData = []\n",
        "for i in range(1,totalfile+1):\n",
        "  path = data_dir + \"/train/\" + \"traindata\" +str(i)+\".json\"\n",
        "  f = open(path,\"r\")\n",
        "  testdata = json.loads(f.read())\n",
        "  f.close()\n",
        "  \n",
        "  arti = testdata['article']\n",
        "  que = testdata['questions'][0]\n",
        "  setData.append(arti)\n",
        "  setData.append(que)\n",
        "\n",
        "setDataEmb = BERTEmbeddings(model,tokenizer,setData)\n",
        "print(setDataEmb.shape)\n",
        "artEmb = setDataEmb[::2]\n",
        "qusEmb = setDataEmb[1::2]\n",
        "print(artEmb.shape)\n",
        "print(qusEmb.shape)\n",
        "saveData = np.concatenate((artEmb,artEmb), axis=1)\n",
        "print(saveData.shape)\n",
        "np.savetxt(basePath+'trainEmbs.csv',saveData, delimiter=',',fmt='%.15f')\n",
        "\n",
        "\n",
        "'''setDataEmb = list(setDataEmb)\n",
        "print(len(setDataEmb),len(setDataEmb[0]),count)\n",
        "l_set = len(setDataEmb)\n",
        "newData = []\n",
        "for i in range(0,l_set,2):\n",
        "  newData.append(np.concatenate([setDataEmb[i],setDataEmb[i+1]]))\n",
        "\n",
        "print(len(newData),len(newData[0]))\n",
        "#np.savetxt(basePath+'testDataEmbs.csv',newData, delimiter=',',fmt='%.15f')\n",
        "np.savetxt(basePath+'trainDataEmbs.csv',newData, delimiter=',',fmt='%.15f')\n",
        "#np.savetxt(basePath+'validDataEmbs.csv',newData, delimiter=',',fmt='%.15f')'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10481\n",
            "Processing  0  out of  20962\n",
            "Processing  500  out of  20962\n",
            "Processing  1000  out of  20962\n",
            "Processing  1500  out of  20962\n",
            "Processing  2000  out of  20962\n",
            "Processing  2500  out of  20962\n",
            "Processing  3000  out of  20962\n",
            "Processing  3500  out of  20962\n",
            "Processing  4000  out of  20962\n",
            "Processing  4500  out of  20962\n",
            "Processing  5000  out of  20962\n",
            "Processing  5500  out of  20962\n",
            "Processing  6000  out of  20962\n",
            "Processing  6500  out of  20962\n",
            "Processing  7000  out of  20962\n",
            "Processing  7500  out of  20962\n",
            "Processing  8000  out of  20962\n",
            "Processing  8500  out of  20962\n",
            "Processing  9000  out of  20962\n",
            "Processing  9500  out of  20962\n",
            "Processing  10000  out of  20962\n",
            "Processing  10500  out of  20962\n",
            "Processing  11000  out of  20962\n",
            "Processing  11500  out of  20962\n",
            "Processing  12000  out of  20962\n",
            "Processing  12500  out of  20962\n",
            "Processing  13000  out of  20962\n",
            "Processing  13500  out of  20962\n",
            "Processing  14000  out of  20962\n",
            "Processing  14500  out of  20962\n",
            "Processing  15000  out of  20962\n",
            "Processing  15500  out of  20962\n",
            "Processing  16000  out of  20962\n",
            "Processing  16500  out of  20962\n",
            "Processing  17000  out of  20962\n",
            "Processing  17500  out of  20962\n",
            "Processing  18000  out of  20962\n",
            "Processing  18500  out of  20962\n",
            "Processing  19000  out of  20962\n",
            "Processing  19500  out of  20962\n",
            "Processing  20000  out of  20962\n",
            "Processing  20500  out of  20962\n",
            "(20962, 768)\n",
            "(10481, 768)\n",
            "(10481, 768)\n",
            "(10481, 1536)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"setDataEmb = list(setDataEmb)\\nprint(len(setDataEmb),len(setDataEmb[0]),count)\\nl_set = len(setDataEmb)\\nnewData = []\\nfor i in range(0,l_set,2):\\n  newData.append(np.concatenate([setDataEmb[i],setDataEmb[i+1]]))\\n\\nprint(len(newData),len(newData[0]))\\n#np.savetxt(basePath+'testDataEmbs.csv',newData, delimiter=',',fmt='%.15f')\\nnp.savetxt(basePath+'trainDataEmbs.csv',newData, delimiter=',',fmt='%.15f')\\n#np.savetxt(basePath+'validDataEmbs.csv',newData, delimiter=',',fmt='%.15f')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_kV5US7UljD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savetxt(basePath+'drive/My Drive/temp/trainEmbs.csv',saveData, delimiter=',',fmt='%.15f')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sZLGqBtpoan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(\"temp\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-lCNlqg6AWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(\"temp\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7lT7TXZ7J7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(\"temp\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}