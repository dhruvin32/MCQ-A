{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_MCQ.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2lGQIIpwJxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "06b6f4d0-6450-485d-c6f6-e0a169c1d1a7"
      },
      "source": [
        "#!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.38.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.4.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.12.40)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.15.40)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.40->boto3->pytorch_pretrained_bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTUUJQP-3TUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!sh setup.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncdFsDJPv0ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import os\n",
        "#import argparse\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "import csv\n",
        "import glob \n",
        "import json\n",
        "#import apex\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertForMultipleChoice\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvE_IuB3x-PP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "basePath = '/content/'\n",
        "\n",
        "data_dir = basePath+'dataset'\n",
        "drive_dir = basePath+'drive/My Drive/temp/DatasetN'\n",
        "\n",
        "'''data_dir = basePath+'tempdataset'\n",
        "drive_dir = basePath+'drive/My Drive/temp/tempDataset' '''\n",
        "\n",
        "bert_model = 'bert-large-uncased'\n",
        "output_dir = basePath+'large_models'\n",
        "\n",
        "max_seq_length = 512\n",
        "do_train = True\n",
        "do_eval = True\n",
        "do_lower_case = True\n",
        "train_batch_size = 1\n",
        "eval_batch_size = 1\n",
        "learning_rate = 0.00001\n",
        "num_train_epochs = 2\n",
        "#-------------------------------------------\n",
        "warmup_proportion = 0.1\n",
        "no_cuda = False  # False = use cuda if available\n",
        "local_rank = -1\n",
        "seed = 42\n",
        "#------------------------------------------\n",
        "gradient_accumulation_steps = 2\n",
        "fp16 = False\n",
        "loss_scale = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DVH8M2tzxgV",
        "colab_type": "code",
        "outputId": "c5e5d6a3-922e-41b2-c4e6-bbca5be5ba97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''import zipfile\n",
        "import os\n",
        "for file_name in os.listdir(drive_dir):\n",
        "  if file_name.endswith('.zip'):\n",
        "    with zipfile.ZipFile(drive_dir+'/'+file_name,'r') as zip_dir:\n",
        "      zip_dir.extractall(path='/content/')'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"import zipfile\\nimport os\\nfor file_name in os.listdir(drive_dir):\\n  if file_name.endswith('.zip'):\\n    with zipfile.ZipFile(drive_dir+'/'+file_name,'r') as zip_dir:\\n      zip_dir.extractall(path='/content/')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtA_xDtWReUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import torch\n",
        "#print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlcKyn1TwBQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCqskyLcwElO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RaceExample(object):\n",
        "    \"\"\"A single training/test example for the RACE dataset.\"\"\"\n",
        "    '''\n",
        "    For RACE dataset:\n",
        "    race_id: data id\n",
        "    context_sentence: article\n",
        "    start_ending: question\n",
        "    ending_0/1/2/3: option_0/1/2/3\n",
        "    label: true answer\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 race_id,\n",
        "                 context_sentence,\n",
        "                 start_ending,\n",
        "                 ending_0,\n",
        "                 ending_1,\n",
        "                 ending_2,\n",
        "                 ending_3,\n",
        "                 label = None):\n",
        "        self.race_id = race_id\n",
        "        self.context_sentence = context_sentence\n",
        "        self.start_ending = start_ending\n",
        "        self.endings = [\n",
        "            ending_0,\n",
        "            ending_1,\n",
        "            ending_2,\n",
        "            ending_3,\n",
        "        ]\n",
        "        self.label = label\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        l = [\n",
        "            f\"id: {self.race_id}\",\n",
        "            f\"article: {self.context_sentence}\",\n",
        "            f\"question: {self.start_ending}\",\n",
        "            f\"option_0: {self.endings[0]}\",\n",
        "            f\"option_1: {self.endings[1]}\",\n",
        "            f\"option_2: {self.endings[2]}\",\n",
        "            f\"option_3: {self.endings[3]}\",\n",
        "        ]\n",
        "\n",
        "        if self.label is not None:\n",
        "            l.append(f\"label: {self.label}\")\n",
        "\n",
        "        return \", \".join(l)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anP7lPY6xkp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    def __init__(self,\n",
        "                 example_id,\n",
        "                 choices_features,\n",
        "                 label\n",
        "\n",
        "    ):\n",
        "        self.example_id = example_id\n",
        "        self.choices_features = [\n",
        "            {\n",
        "                'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'segment_ids': segment_ids\n",
        "            }\n",
        "            for _, input_ids, input_mask, segment_ids in choices_features\n",
        "        ]\n",
        "        self.label = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpmlldL_xnh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## paths is a list containing all paths\n",
        "def read_race_examples(paths):\n",
        "    examples = []\n",
        "    for path in paths:\n",
        "        filenames = glob.glob(path+\"/*json\")\n",
        "        for filename in filenames:\n",
        "            with open(filename, 'r', encoding='utf-8') as fpr:\n",
        "                data_raw = json.load(fpr)\n",
        "                article = data_raw['article']\n",
        "                ## for each qn\n",
        "                for i in range(len(data_raw['answers'])):\n",
        "                    truth = ord(data_raw['answers'][i]) - ord('A')\n",
        "                    question = data_raw['questions'][i]\n",
        "                    options = data_raw['options'][i]\n",
        "                    examples.append(\n",
        "                        RaceExample(\n",
        "                            race_id = filename+'-'+str(i),\n",
        "                            context_sentence = article,\n",
        "                            start_ending = question,\n",
        "\n",
        "                            ending_0 = options[0],\n",
        "                            ending_1 = options[1],\n",
        "                            ending_2 = options[2],\n",
        "                            ending_3 = options[3],\n",
        "                            label = truth))\n",
        "                \n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWHk7eBxsog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
        "                                 is_training):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    # RACE is a multiple choice task. To perform this task using Bert,\n",
        "    # we will use the formatting proposed in \"Improving Language\n",
        "    # Understanding by Generative Pre-Training\" and suggested by\n",
        "    # @jacobdevlin-google in this issue\n",
        "    # https://github.com/google-research/bert/issues/38.\n",
        "    #\n",
        "    # The input will be like:\n",
        "    # [CLS] Article [SEP] Question + Option [SEP]\n",
        "    # for each option \n",
        "    # \n",
        "    # The model will output a single value for each input. To get the\n",
        "    # final decision of the model, we will run a softmax over these 4\n",
        "    # outputs.\n",
        "    features = []\n",
        "    for example_index, example in enumerate(examples):\n",
        "        context_tokens = tokenizer.tokenize(example.context_sentence)\n",
        "        start_ending_tokens = tokenizer.tokenize(example.start_ending)\n",
        "\n",
        "        choices_features = []\n",
        "        for ending_index, ending in enumerate(example.endings):\n",
        "            # We create a copy of the context tokens in order to be\n",
        "            # able to shrink it according to ending_tokens\n",
        "            context_tokens_choice = context_tokens[:]\n",
        "            ending_tokens = start_ending_tokens + tokenizer.tokenize(ending)\n",
        "            # Modifies `context_tokens_choice` and `ending_tokens` in\n",
        "            # place so that the total length is less than the\n",
        "            # specified length.  Account for [CLS], [SEP], [SEP] with\n",
        "            # \"- 3\"\n",
        "            _truncate_seq_pair(context_tokens_choice, ending_tokens, max_seq_length - 3)\n",
        "\n",
        "            tokens = [\"[CLS]\"] + context_tokens_choice + [\"[SEP]\"] + ending_tokens + [\"[SEP]\"]\n",
        "            segment_ids = [0] * (len(context_tokens_choice) + 2) + [1] * (len(ending_tokens) + 1)\n",
        "\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "            input_mask = [1] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding = [0] * (max_seq_length - len(input_ids))\n",
        "            input_ids += padding\n",
        "            input_mask += padding\n",
        "            segment_ids += padding\n",
        "\n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segment_ids) == max_seq_length\n",
        "\n",
        "            choices_features.append((tokens, input_ids, input_mask, segment_ids))\n",
        "\n",
        "        label = example.label\n",
        "        ## display some example\n",
        "        if example_index < 1:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(f\"race_id: {example.race_id}\")\n",
        "            for choice_idx, (tokens, input_ids, input_mask, segment_ids) in enumerate(choices_features):\n",
        "                logger.info(f\"choice: {choice_idx}\")\n",
        "                logger.info(f\"tokens: {' '.join(tokens)}\")\n",
        "                logger.info(f\"input_ids: {' '.join(map(str, input_ids))}\")\n",
        "                logger.info(f\"input_mask: {' '.join(map(str, input_mask))}\")\n",
        "                logger.info(f\"segment_ids: {' '.join(map(str, segment_ids))}\")\n",
        "            if is_training:\n",
        "                logger.info(f\"label: {label}\")\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                example_id = example.race_id,\n",
        "                choices_features = choices_features,\n",
        "                label = label\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwcBWSyAxulF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T16BaD2Yxwge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(out, labels):\n",
        "    outputs = np.argmax(out, axis=1)\n",
        "    return np.sum(outputs == labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB_sayHExyF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def select_field(features, field):\n",
        "    return [\n",
        "        [\n",
        "            choice[field]\n",
        "            for choice in feature.choices_features\n",
        "        ]\n",
        "        for feature in features\n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Sr4oXOxzrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0 - x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9jTTz1zx12f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    global train_batch_size\n",
        "\n",
        "    if local_rank == -1 or no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "    else:\n",
        "        torch.cuda.set_device(local_rank)\n",
        "        device = torch.device(\"cuda\", local_rank)\n",
        "        n_gpu = 1\n",
        "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
        "        device, n_gpu, bool(local_rank != -1), fp16))\n",
        "\n",
        "    if gradient_accumulation_steps < 1:\n",
        "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
        "                            gradient_accumulation_steps))\n",
        "\n",
        "    train_batch_size = int(train_batch_size / gradient_accumulation_steps)\n",
        "    if train_batch_size == 0:\n",
        "        train_batch_size = 1\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    if not do_train and not do_eval:\n",
        "        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
        "\n",
        "    if os.path.exists(output_dir) and os.listdir(output_dir):\n",
        "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(output_dir))\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n",
        "\n",
        "    train_examples = None\n",
        "    num_train_steps = None\n",
        "    if do_train:\n",
        "        train_dir = os.path.join(data_dir, 'train')\n",
        "        eval_dir = os.path.join(data_dir, 'dev')\n",
        "        train_examples = read_race_examples([train_dir,eval_dir])\n",
        "        #train_examples = read_race_examples([train_dir])\n",
        "        print(train_batch_size,gradient_accumulation_steps,num_train_epochs)\n",
        "        num_train_steps = int(\n",
        "            len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)\n",
        "\n",
        "    # Prepare model\n",
        "    model = BertForMultipleChoice.from_pretrained(bert_model,\n",
        "        cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(local_rank),\n",
        "        num_choices=4)\n",
        "    \n",
        "    \n",
        "    '''model_state_dict = torch.load('/content/drive/My Drive/temp/Dataset/large_model.bin')\n",
        "    model = BertForMultipleChoice.from_pretrained(bert_model,state_dict=model_state_dict,num_choices=4)'''\n",
        "\n",
        "    if fp16:\n",
        "        model.half()\n",
        "    model.to(device)\n",
        "    if local_rank != -1:\n",
        "        try:\n",
        "            from apex.parallel import DistributedDataParallel as DDP\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "\n",
        "        model = DDP(model)\n",
        "    elif n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Prepare optimizer\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "\n",
        "    # hack to remove pooler, which is not used\n",
        "    # thus it produce None grad that break apex\n",
        "    param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    t_total = num_train_steps\n",
        "    if local_rank != -1:\n",
        "        t_total = t_total // torch.distributed.get_world_size()\n",
        "    if fp16:\n",
        "        try:\n",
        "            #from apex.optimizers import FP16_Optimizer\n",
        "            from apex.fp16_utils.fp16_optimizer import FP16_Optimizer\n",
        "            from apex.optimizers import FusedAdam\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "\n",
        "        optimizer = FusedAdam(optimizer_grouped_parameters,\n",
        "                              lr=learning_rate,\n",
        "                              bias_correction=False)\n",
        "        if loss_scale == 0:\n",
        "            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
        "        else:\n",
        "            optimizer = FP16_Optimizer(optimizer, static_loss_scale=loss_scale)\n",
        "    else:\n",
        "        optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                             lr=learning_rate,\n",
        "                             warmup=warmup_proportion,\n",
        "                             t_total=t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    if do_train:\n",
        "        train_features = convert_examples_to_features(\n",
        "            train_examples, tokenizer, max_seq_length, True)\n",
        "        logger.info(\"***** Running training *****\")\n",
        "        logger.info(\"  Num examples = %d\", len(train_examples))\n",
        "        logger.info(\"  Batch size = %d\", train_batch_size)\n",
        "        logger.info(\"  Num steps = %d\", num_train_steps)\n",
        "        all_input_ids = torch.tensor(select_field(train_features, 'input_ids'), dtype=torch.long)\n",
        "        all_input_mask = torch.tensor(select_field(train_features, 'input_mask'), dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor(select_field(train_features, 'segment_ids'), dtype=torch.long)\n",
        "        all_label = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
        "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
        "        if local_rank == -1:\n",
        "            train_sampler = RandomSampler(train_data)\n",
        "        else:\n",
        "            train_sampler = DistributedSampler(train_data)\n",
        "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
        "\n",
        "        model.train()\n",
        "        for ep in range(int(num_train_epochs)):\n",
        "            tr_loss = 0\n",
        "            nb_tr_examples, nb_tr_steps = 0, 0\n",
        "            logger.info(\"Trianing Epoch: {}/{}\".format(ep+1, int(num_train_epochs)))\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                #print(\"-----**--------------Enter-----------**------------\")\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                input_ids, input_mask, segment_ids, label_ids = batch\n",
        "                loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "                #print('loss:',loss)\n",
        "                if n_gpu > 1:\n",
        "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
        "                if fp16 and loss_scale != 1.0:\n",
        "                    # rescale loss for fp16 training\n",
        "                    # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
        "                    loss = loss * loss_scale\n",
        "                #print('loss:',loss)\n",
        "                if gradient_accumulation_steps > 1:\n",
        "                    loss = loss / gradient_accumulation_steps\n",
        "                #print('loss:',loss)\n",
        "                tr_loss += loss.item()\n",
        "                #print('loss:',tr_loss)\n",
        "                nb_tr_examples += input_ids.size(0)\n",
        "                nb_tr_steps += 1\n",
        "                #print('nb_tr_steps:', nb_tr_steps)\n",
        "\n",
        "                if fp16:\n",
        "                    optimizer.backward(loss)\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                    # modify learning rate with special warm up BERT uses\n",
        "                    lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] = lr_this_step\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "                if global_step%100 == 0:\n",
        "                    logger.info(\"Training loss: {}, global step: {}\".format(tr_loss/nb_tr_steps, global_step))\n",
        "\n",
        "\n",
        "\n",
        "    # Save a trained model\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "\n",
        "    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "    torch.save(model_to_save.state_dict(), output_model_file)\n",
        "\n",
        "    output_model_file = os.path.join(drive_dir, \"pytorch_model2.bin\")\n",
        "    torch.save(model_to_save.state_dict(), output_model_file)\n",
        "\n",
        "\n",
        "    ## Load a trained model that you have fine-tuned\n",
        "    ## use this part if you want to load the trained model\n",
        "    # model_state_dict = torch.load(output_model_file)\n",
        "    # model = BertForMultipleChoice.from_pretrained(bert_model,\n",
        "    #     state_dict=model_state_dict,\n",
        "    #     num_choices=4)\n",
        "    # model.to(device)\n",
        "\n",
        "    if do_eval and (local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "        test_dir = os.path.join(data_dir, 'test')\n",
        "        test_high = [test_dir ]\n",
        "        #test_middle = [test_dir + '/middle']\n",
        "\n",
        "        ## test high \n",
        "        eval_examples = read_race_examples(test_high)\n",
        "        eval_features = convert_examples_to_features(\n",
        "            eval_examples, tokenizer, max_seq_length, True)\n",
        "        logger.info(\"***** Running evaluation: test high *****\")\n",
        "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
        "        logger.info(\"  Batch size = %d\", eval_batch_size)\n",
        "        all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long)\n",
        "        all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long)\n",
        "        all_label = torch.tensor([f.label for f in eval_features], dtype=torch.long)\n",
        "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
        "        # Run prediction for full data\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
        "\n",
        "        model.eval()\n",
        "        high_eval_loss, high_eval_accuracy = 0, 0\n",
        "        high_nb_eval_steps, high_nb_eval_examples = 0, 0\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, input_mask, segment_ids, label_ids = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "                logits = model(input_ids, segment_ids, input_mask)\n",
        "\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = label_ids.to('cpu').numpy()\n",
        "            tmp_eval_accuracy = accuracy(logits, label_ids)\n",
        "\n",
        "            high_eval_loss += tmp_eval_loss.mean().item()\n",
        "            high_eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            high_nb_eval_examples += input_ids.size(0)\n",
        "            high_nb_eval_steps += 1\n",
        "\n",
        "        eval_loss = high_eval_loss / high_nb_eval_steps\n",
        "        eval_accuracy = high_eval_accuracy / high_nb_eval_examples\n",
        "\n",
        "        result = {'high_eval_loss': eval_loss,\n",
        "                  'high_eval_accuracy': eval_accuracy}\n",
        "\n",
        "        output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"a+\") as writer:\n",
        "            logger.info(\"***** Eval results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIBYPiqJzrin",
        "colab_type": "code",
        "outputId": "5054c458-7474-467a-815c-550ffbae3012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/24/2020 07:34:00 - INFO - __main__ -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/24/2020 07:34:00 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/24/2020 07:34:01 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz not found in cache, downloading to /tmp/tmptgzx5a8r\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 2 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1248501532/1248501532 [00:21<00:00, 58644908.78B/s]\n",
            "04/24/2020 07:34:23 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmptgzx5a8r to cache at /root/.pytorch_pretrained_bert/distributed_-1/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05\n",
            "04/24/2020 07:34:26 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/distributed_-1/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05\n",
            "04/24/2020 07:34:26 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmptgzx5a8r\n",
            "04/24/2020 07:34:27 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/distributed_-1/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05\n",
            "04/24/2020 07:34:27 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/distributed_-1/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05 to temp dir /tmp/tmp1q32qro3\n",
            "04/24/2020 07:34:48 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/24/2020 07:34:59 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForMultipleChoice not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "04/24/2020 07:34:59 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   *** Example ***\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   race_id: /content/dataset/train/traindata918.json-0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   choice: 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   tokens: [CLS] ge ##yse ##rs are also created by water that is heated beneath the earth ’ s surface . the water may become super ##hea ##ted by magma . it becomes trapped in a narrow passageway . the heat and pressure build as more water is added . when the pressure is too much , the super ##hea ##ted water bursts out onto the surface . this is a ge ##yse ##r . [SEP] ge ##yse ##rs are created when what is heated beneath the earth ' s surface ? magma [SEP]\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   input_ids: 101 16216 23274 2869 2024 2036 2580 2011 2300 2008 2003 9685 4218 1996 3011 1521 1055 3302 1012 1996 2300 2089 2468 3565 20192 3064 2011 28933 1012 2009 4150 7567 1999 1037 4867 27336 1012 1996 3684 1998 3778 3857 2004 2062 2300 2003 2794 1012 2043 1996 3778 2003 2205 2172 1010 1996 3565 20192 3064 2300 19239 2041 3031 1996 3302 1012 2023 2003 1037 16216 23274 2099 1012 102 16216 23274 2869 2024 2580 2043 2054 2003 9685 4218 1996 3011 1005 1055 3302 1029 28933 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   choice: 1\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   tokens: [CLS] ge ##yse ##rs are also created by water that is heated beneath the earth ’ s surface . the water may become super ##hea ##ted by magma . it becomes trapped in a narrow passageway . the heat and pressure build as more water is added . when the pressure is too much , the super ##hea ##ted water bursts out onto the surface . this is a ge ##yse ##r . [SEP] ge ##yse ##rs are created when what is heated beneath the earth ' s surface ? water [SEP]\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   input_ids: 101 16216 23274 2869 2024 2036 2580 2011 2300 2008 2003 9685 4218 1996 3011 1521 1055 3302 1012 1996 2300 2089 2468 3565 20192 3064 2011 28933 1012 2009 4150 7567 1999 1037 4867 27336 1012 1996 3684 1998 3778 3857 2004 2062 2300 2003 2794 1012 2043 1996 3778 2003 2205 2172 1010 1996 3565 20192 3064 2300 19239 2041 3031 1996 3302 1012 2023 2003 1037 16216 23274 2099 1012 102 16216 23274 2869 2024 2580 2043 2054 2003 9685 4218 1996 3011 1005 1055 3302 1029 2300 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   choice: 2\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   tokens: [CLS] ge ##yse ##rs are also created by water that is heated beneath the earth ’ s surface . the water may become super ##hea ##ted by magma . it becomes trapped in a narrow passageway . the heat and pressure build as more water is added . when the pressure is too much , the super ##hea ##ted water bursts out onto the surface . this is a ge ##yse ##r . [SEP] ge ##yse ##rs are created when what is heated beneath the earth ' s surface ? air [SEP]\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   input_ids: 101 16216 23274 2869 2024 2036 2580 2011 2300 2008 2003 9685 4218 1996 3011 1521 1055 3302 1012 1996 2300 2089 2468 3565 20192 3064 2011 28933 1012 2009 4150 7567 1999 1037 4867 27336 1012 1996 3684 1998 3778 3857 2004 2062 2300 2003 2794 1012 2043 1996 3778 2003 2205 2172 1010 1996 3565 20192 3064 2300 19239 2041 3031 1996 3302 1012 2023 2003 1037 16216 23274 2099 1012 102 16216 23274 2869 2024 2580 2043 2054 2003 9685 4218 1996 3011 1005 1055 3302 1029 2250 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   choice: 3\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   tokens: [CLS] ge ##yse ##rs are also created by water that is heated beneath the earth ’ s surface . the water may become super ##hea ##ted by magma . it becomes trapped in a narrow passageway . the heat and pressure build as more water is added . when the pressure is too much , the super ##hea ##ted water bursts out onto the surface . this is a ge ##yse ##r . [SEP] ge ##yse ##rs are created when what is heated beneath the earth ' s surface ? gases [SEP]\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   input_ids: 101 16216 23274 2869 2024 2036 2580 2011 2300 2008 2003 9685 4218 1996 3011 1521 1055 3302 1012 1996 2300 2089 2468 3565 20192 3064 2011 28933 1012 2009 4150 7567 1999 1037 4867 27336 1012 1996 3684 1998 3778 3857 2004 2062 2300 2003 2794 1012 2043 1996 3778 2003 2205 2172 1010 1996 3565 20192 3064 2300 19239 2041 3031 1996 3302 1012 2023 2003 1037 16216 23274 2099 1012 102 16216 23274 2869 2024 2580 2043 2054 2003 9685 4218 1996 3011 1005 1055 3302 1029 15865 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/24/2020 07:34:59 - INFO - __main__ -   label: 1\n",
            "04/24/2020 07:35:21 - INFO - __main__ -   ***** Running training *****\n",
            "04/24/2020 07:35:21 - INFO - __main__ -     Num examples = 11368\n",
            "04/24/2020 07:35:21 - INFO - __main__ -     Batch size = 1\n",
            "04/24/2020 07:35:21 - INFO - __main__ -     Num steps = 11368\n",
            "04/24/2020 07:35:23 - INFO - __main__ -   Trianing Epoch: 1/2\n",
            "04/24/2020 07:35:34 - INFO - __main__ -   Training loss: 0.7338823080062866, global step: 0\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}