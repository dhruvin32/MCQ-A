{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLNET_MCQ.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2lGQIIpwJxY",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''!pip install pytorch_pretrained_bert\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tRuenJttfWuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncdFsDJPv0ed",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import os\n",
        "#import argparse\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "import csv\n",
        "import glob \n",
        "import json\n",
        "#import apex\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from transformers import *\n",
        "\n",
        "from pytorch_pretrained_bert.optimization import BertAdam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvE_IuB3x-PP",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "basePath = '/kaggle/input/allen-a12-science-challenge/'\n",
        "\n",
        "data_dir = basePath+'dataset/dataset'\n",
        "#drive_dir = basePath+'drive/My Drive/temp/Dataset'\n",
        "\n",
        "'''data_dir = basePath+'tempdataset'\n",
        "drive_dir = basePath+'drive/My Drive/temp/tempDataset' '''\n",
        "\n",
        "# roberta_model = 'roberta-base'\n",
        "xlnet_model = 'xlnet-base-cased'\n",
        "output_dir = '/kaggle/working/'\n",
        "\n",
        "max_seq_length = 512\n",
        "do_train = True\n",
        "do_eval = True\n",
        "do_lower_case = True\n",
        "train_batch_size = 1\n",
        "eval_batch_size = 1\n",
        "learning_rate = 0.000001\n",
        "num_train_epochs = 1\n",
        "#-------------------------------------------\n",
        "warmup_proportion = 0.1\n",
        "no_cuda = False  # False = use cuda if available\n",
        "local_rank = -1\n",
        "seed = 42\n",
        "#------------------------------------------\n",
        "gradient_accumulation_steps = 2\n",
        "fp16 = False\n",
        "loss_scale = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Aljgt96SfWuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenizer = RobertaTokenizer.from_pretrained(roberta_model, do_lower_case=do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DVH8M2tzxgV",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''import zipfile\n",
        "import os\n",
        "for file_name in os.listdir(drive_dir):\n",
        "  if file_name.endswith('.zip'):\n",
        "    with zipfile.ZipFile(drive_dir+'/'+file_name,'r') as zip_dir:\n",
        "      zip_dir.extractall(path='/content/')'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtA_xDtWReUO",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import torch\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlcKyn1TwBQI",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCqskyLcwElO",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RaceExample(object):\n",
        "    \"\"\"A single training/test example for the RACE dataset.\"\"\"\n",
        "    '''\n",
        "    For RACE dataset:\n",
        "    race_id: data id\n",
        "    context_sentence: article\n",
        "    start_ending: question\n",
        "    ending_0/1/2/3: option_0/1/2/3\n",
        "    label: true answer\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 race_id,\n",
        "                 context_sentence,\n",
        "                 start_ending,\n",
        "                 ending_0,\n",
        "                 ending_1,\n",
        "                 ending_2,\n",
        "                 ending_3,\n",
        "                 label = None):\n",
        "        self.race_id = race_id\n",
        "        self.context_sentence = context_sentence\n",
        "        self.start_ending = start_ending\n",
        "        self.endings = [\n",
        "            ending_0,\n",
        "            ending_1,\n",
        "            ending_2,\n",
        "            ending_3,\n",
        "        ]\n",
        "        self.label = label\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        l = [\n",
        "            f\"id: {self.race_id}\",\n",
        "            f\"article: {self.context_sentence}\",\n",
        "            f\"question: {self.start_ending}\",\n",
        "            f\"option_0: {self.endings[0]}\",\n",
        "            f\"option_1: {self.endings[1]}\",\n",
        "            f\"option_2: {self.endings[2]}\",\n",
        "            f\"option_3: {self.endings[3]}\",\n",
        "        ]\n",
        "\n",
        "        if self.label is not None:\n",
        "            l.append(f\"label: {self.label}\")\n",
        "\n",
        "        return \", \".join(l)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anP7lPY6xkp_",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "    def __init__(self,\n",
        "                 example_id,\n",
        "                 choices_features,\n",
        "                 label\n",
        "\n",
        "    ):\n",
        "        self.example_id = example_id\n",
        "        self.choices_features = [\n",
        "            {\n",
        "                'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'segment_ids': segment_ids\n",
        "            }\n",
        "            for _, input_ids, input_mask, segment_ids in choices_features\n",
        "        ]\n",
        "        self.label = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpmlldL_xnh1",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## paths is a list containing all paths\n",
        "def read_race_examples(paths):\n",
        "    examples = []\n",
        "    for path in paths:\n",
        "        filenames = glob.glob(path+\"/*json\")\n",
        "        for filename in filenames:\n",
        "            with open(filename, 'r', encoding='utf-8') as fpr:\n",
        "                data_raw = json.load(fpr)\n",
        "                article = data_raw['article']\n",
        "                ## for each qn\n",
        "                for i in range(len(data_raw['answers'])):\n",
        "                    truth = ord(data_raw['answers'][i]) - ord('A')\n",
        "                    question = data_raw['questions'][i]\n",
        "                    options = data_raw['options'][i]\n",
        "                    examples.append(\n",
        "                        RaceExample(\n",
        "                            race_id = filename+'-'+str(i),\n",
        "                            context_sentence = article,\n",
        "                            start_ending = question,\n",
        "\n",
        "                            ending_0 = options[0],\n",
        "                            ending_1 = options[1],\n",
        "                            ending_2 = options[2],\n",
        "                            ending_3 = options[3],\n",
        "                            label = truth))\n",
        "                \n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBWHk7eBxsog",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
        "                                 is_training):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    # RACE is a multiple choice task. To perform this task using Bert,\n",
        "    # we will use the formatting proposed in \"Improving Language\n",
        "    # Understanding by Generative Pre-Training\" and suggested by\n",
        "    # @jacobdevlin-google in this issue\n",
        "    # https://github.com/google-research/bert/issues/38.\n",
        "    #\n",
        "    # The input will be like:\n",
        "    # [CLS] Article [SEP] Question + Option [SEP]\n",
        "    # for each option \n",
        "    # \n",
        "    # The model will output a single value for each input. To get the\n",
        "    # final decision of the model, we will run a softmax over these 4\n",
        "    # outputs.\n",
        "    features = []\n",
        "    for example_index, example in enumerate(examples):\n",
        "        context_tokens = tokenizer.tokenize(example.context_sentence)\n",
        "        start_ending_tokens = tokenizer.tokenize(example.start_ending)\n",
        "\n",
        "        choices_features = []\n",
        "        for ending_index, ending in enumerate(example.endings):\n",
        "            # We create a copy of the context tokens in order to be\n",
        "            # able to shrink it according to ending_tokens\n",
        "            context_tokens_choice = context_tokens[:]\n",
        "            ending_tokens = start_ending_tokens + tokenizer.tokenize(ending)\n",
        "            # Modifies `context_tokens_choice` and `ending_tokens` in\n",
        "            # place so that the total length is less than the\n",
        "            # specified length.  Account for [CLS], [SEP], [SEP] with\n",
        "            # \"- 3\"\n",
        "            _truncate_seq_pair(context_tokens_choice, ending_tokens, max_seq_length - 3)\n",
        "\n",
        "            tokens = [tokenizer.cls_token] + context_tokens_choice + [tokenizer.sep_token] + ending_tokens + [tokenizer.sep_token]\n",
        "            segment_ids = [0] * (len(context_tokens_choice) + 2) + [1] * (len(ending_tokens) + 1)\n",
        "\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "            input_mask = [1] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding = [0] * (max_seq_length - len(input_ids))\n",
        "            input_ids += padding\n",
        "            input_mask += padding\n",
        "            segment_ids += padding\n",
        "\n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segment_ids) == max_seq_length\n",
        "\n",
        "            choices_features.append((tokens, input_ids, input_mask, segment_ids))\n",
        "\n",
        "        label = example.label\n",
        "        ## display some example\n",
        "        if example_index < 1:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(f\"race_id: {example.race_id}\")\n",
        "            for choice_idx, (tokens, input_ids, input_mask, segment_ids) in enumerate(choices_features):\n",
        "                logger.info(f\"choice: {choice_idx}\")\n",
        "                logger.info(f\"tokens: {' '.join(tokens)}\")\n",
        "                logger.info(f\"input_ids: {' '.join(map(str, input_ids))}\")\n",
        "                logger.info(f\"input_mask: {' '.join(map(str, input_mask))}\")\n",
        "                logger.info(f\"segment_ids: {' '.join(map(str, segment_ids))}\")\n",
        "            if is_training:\n",
        "                logger.info(f\"label: {label}\")\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                example_id = example.race_id,\n",
        "                choices_features = choices_features,\n",
        "                label = label\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwcBWSyAxulF",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T16BaD2Yxwge",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(out, labels):\n",
        "    outputs = np.argmax(out, axis=1)\n",
        "    return np.sum(outputs == labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB_sayHExyF1",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def select_field(features, field):\n",
        "    return [\n",
        "        [\n",
        "            choice[field]\n",
        "            for choice in feature.choices_features\n",
        "        ]\n",
        "        for feature in features\n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Sr4oXOxzrm",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0 - x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9jTTz1zx12f",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    global train_batch_size\n",
        "\n",
        "    if local_rank == -1 or no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "    else:\n",
        "        torch.cuda.set_device(local_rank)\n",
        "        device = torch.device(\"cuda\", local_rank)\n",
        "        n_gpu = 1\n",
        "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
        "        device, n_gpu, bool(local_rank != -1), fp16))\n",
        "\n",
        "    if gradient_accumulation_steps < 1:\n",
        "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
        "                            gradient_accumulation_steps))\n",
        "\n",
        "    train_batch_size = int(train_batch_size / gradient_accumulation_steps)\n",
        "    if train_batch_size == 0:\n",
        "        train_batch_size = 1\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    if not do_train and not do_eval:\n",
        "        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        tokenizer = XLNetTokenizer.from_pretrained(xlnet_model, do_lower_case=do_lower_case)\n",
        "\n",
        "    train_examples = None\n",
        "    num_train_steps = None\n",
        "    if do_train:\n",
        "        train_dir = os.path.join(data_dir, 'train')\n",
        "        eval_dir = os.path.join(data_dir, 'dev')\n",
        "        train_examples = read_race_examples([train_dir,eval_dir])\n",
        "        #print(train_batch_size,gradient_accumulation_steps,num_train_epochs)\n",
        "        num_train_steps = int(\n",
        "            len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)\n",
        "\n",
        "    # Prepare model\n",
        "    model_state_dict = torch.load(basePath+'pytorch_model_xlnet_epoch2.bin')\n",
        "    model = XLNetForMultipleChoice.from_pretrained(xlnet_model,state_dict=model_state_dict)\n",
        "    \n",
        "    #model = XLNetForMultipleChoice.from_pretrained(xlnet_model)\n",
        "    \n",
        "\n",
        "    if fp16:\n",
        "        model.half()\n",
        "    model.to(device)\n",
        "    if local_rank != -1:\n",
        "        try:\n",
        "            from apex.parallel import DistributedDataParallel as DDP\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "\n",
        "        model = DDP(model)\n",
        "    elif n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Prepare optimizer\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "\n",
        "    # hack to remove pooler, which is not used\n",
        "    # thus it produce None grad that break apex\n",
        "    param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    t_total = num_train_steps\n",
        "    if local_rank != -1:\n",
        "        t_total = t_total // torch.distributed.get_world_size()\n",
        "    if fp16:\n",
        "        try:\n",
        "            #from apex.optimizers import FP16_Optimizer\n",
        "            from apex.fp16_utils.fp16_optimizer import FP16_Optimizer\n",
        "            from apex.optimizers import FusedAdam\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "\n",
        "        optimizer = FusedAdam(optimizer_grouped_parameters,\n",
        "                              lr=learning_rate,\n",
        "                              bias_correction=False)\n",
        "        if loss_scale == 0:\n",
        "            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
        "        else:\n",
        "            optimizer = FP16_Optimizer(optimizer, static_loss_scale=loss_scale)\n",
        "    else:\n",
        "        optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                             lr=learning_rate,\n",
        "                             warmup=warmup_proportion,\n",
        "                             t_total=t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    if do_train:\n",
        "        train_features = convert_examples_to_features(\n",
        "            train_examples, tokenizer, max_seq_length, True)\n",
        "        logger.info(\"***** Running training *****\")\n",
        "        logger.info(\"  Num examples = %d\", len(train_examples))\n",
        "        logger.info(\"  Batch size = %d\", train_batch_size)\n",
        "        logger.info(\"  Num steps = %d\", num_train_steps)\n",
        "        print('Num steps: ',num_train_steps)\n",
        "        all_input_ids = torch.tensor(select_field(train_features, 'input_ids'), dtype=torch.long)\n",
        "        all_input_mask = torch.tensor(select_field(train_features, 'input_mask'), dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor(select_field(train_features, 'segment_ids'), dtype=torch.long)\n",
        "        all_label = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
        "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
        "        if local_rank == -1:\n",
        "            train_sampler = RandomSampler(train_data)\n",
        "        else:\n",
        "            train_sampler = DistributedSampler(train_data)\n",
        "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
        "\n",
        "        model.train()\n",
        "        for ep in range(int(num_train_epochs)):\n",
        "            tr_loss = 0\n",
        "            nb_tr_examples, nb_tr_steps = 0, 0\n",
        "            logger.info(\"Trianing Epoch: {}/{}\".format(ep+1, int(num_train_epochs)))\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                input_ids, input_mask, segment_ids, label_ids = batch\n",
        "                loss,_ = model(input_ids = input_ids, attention_mask = input_mask,labels = label_ids)\n",
        "                if n_gpu > 1:\n",
        "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
        "                if fp16 and loss_scale != 1.0:\n",
        "                    # rescale loss for fp16 training\n",
        "                    # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
        "                    loss = loss * loss_scale\n",
        "                if gradient_accumulation_steps > 1:\n",
        "                    loss = loss / gradient_accumulation_steps\n",
        "                tr_loss += loss.item()\n",
        "                nb_tr_examples += input_ids.size(0)\n",
        "                nb_tr_steps += 1\n",
        "\n",
        "                if fp16:\n",
        "                    optimizer.backward(loss)\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                    # modify learning rate with special warm up BERT uses\n",
        "                    lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] = lr_this_step\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "                if global_step%100 == 0:\n",
        "                    logger.info(\"Training loss: {}, global step: {}\".format(tr_loss/nb_tr_steps, global_step))\n",
        "                    print('loss: ',tr_loss/nb_tr_steps)\n",
        "                    print('global step: ',global_step)\n",
        "\n",
        "\n",
        "\n",
        "    # Save a trained model\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "\n",
        "    output_model_file = os.path.join(output_dir, \"pytorch_model_xlnet_epoch3.bin\")\n",
        "    torch.save(model_to_save.state_dict(), output_model_file)\n",
        "\n",
        "    '''output_model_file = os.path.join(drive_dir, \"pytorch_model.bin\")\n",
        "    torch.save(model_to_save.state_dict(), output_model_file)'''\n",
        "    \n",
        "\n",
        "\n",
        "    ## Load a trained model that you have fine-tuned\n",
        "    ## use this part if you want to load the trained model\n",
        "    '''model_state_dict = torch.load(basePath+'epoch3_Robert_base.bin')\n",
        "    model = RobertaForMultipleChoice.from_pretrained(roberta_model,state_dict=model_state_dict)\n",
        "    model.to(device)\n",
        "    print(\"model loaded...\")'''\n",
        "\n",
        "    if do_eval and (local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "        test_dir = os.path.join(data_dir, 'test')\n",
        "        test_high = [test_dir ]\n",
        "\n",
        "        eval_examples = read_race_examples(test_high)\n",
        "        eval_features = convert_examples_to_features(\n",
        "            eval_examples, tokenizer, max_seq_length, True)\n",
        "        logger.info(\"***** Running evaluation: test high *****\")\n",
        "        print(\"***** Running evaluation: test high *****\")\n",
        "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
        "        print('Num examples: ',len(eval_examples))\n",
        "        logger.info(\"  Batch size = %d\", eval_batch_size)\n",
        "        all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long)\n",
        "        all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long)\n",
        "        all_label = torch.tensor([f.label for f in eval_features], dtype=torch.long)\n",
        "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
        "        # Run prediction for full data\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
        "\n",
        "        model.eval()\n",
        "        high_eval_loss, high_eval_accuracy = 0, 0\n",
        "        high_nb_eval_steps, high_nb_eval_examples = 0, 0\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, input_mask, segment_ids, label_ids = batch\n",
        "\n",
        "            with torch.no_grad():\n",
        "                tmp_eval_loss, _ = model(input_ids = input_ids, attention_mask = input_mask,labels = label_ids)\n",
        "                logits = model(input_ids = input_ids,attention_mask =  input_mask)\n",
        "\n",
        "            logits = logits[0].detach().cpu().numpy()\n",
        "            label_ids = label_ids.to('cpu').numpy()\n",
        "            tmp_eval_accuracy = accuracy(logits, label_ids)\n",
        "\n",
        "            high_eval_loss += tmp_eval_loss.mean().item()\n",
        "            high_eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            high_nb_eval_examples += input_ids.size(0)\n",
        "            high_nb_eval_steps += 1\n",
        "\n",
        "        eval_loss = high_eval_loss / high_nb_eval_steps\n",
        "        eval_accuracy = high_eval_accuracy / high_nb_eval_examples\n",
        "\n",
        "        result = {'high_eval_loss': eval_loss,\n",
        "                  'high_eval_accuracy': eval_accuracy}\n",
        "\n",
        "        output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"a+\") as writer:\n",
        "            logger.info(\"***** Eval results *****\")\n",
        "            for key in sorted(result.keys()):\n",
        "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                print(key,' = ',str(result[key]))\n",
        "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "DH4a1rSBfWvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}