{"cells":[{"metadata":{"id":"S2lGQIIpwJxY","outputId":"9d2731ab-e103-4469-f9e8-a6ede1dc7a96","trusted":true},"cell_type":"code","source":"'''!pip install pytorch_pretrained_bert\n!git clone https://github.com/NVIDIA/apex\n!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"id":"ncdFsDJPv0ed","trusted":true},"cell_type":"code","source":"import logging\nimport os\n#import argparse\nimport random\nfrom tqdm import tqdm, trange\nimport csv\nimport glob \nimport json\n#import apex\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom transformers import *\n\nfrom pytorch_pretrained_bert.optimization import BertAdam\n'''from pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertForMultipleChoice\nfrom pytorch_pretrained_bert.optimization import BertAdam\nfrom pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE'''","execution_count":null,"outputs":[]},{"metadata":{"id":"lvE_IuB3x-PP","trusted":true},"cell_type":"code","source":"basePath = '/kaggle/input/allen-a12-science-challenge/'\n\ndata_dir = basePath+'dataset/dataset'\n#drive_dir = basePath+'drive/My Drive/temp/Dataset'\n\n'''data_dir = basePath+'tempdataset'\ndrive_dir = basePath+'drive/My Drive/temp/tempDataset' '''\n\nroberta_model = 'roberta-base'\noutput_dir = '/kaggle/working/large_models'\n\nmax_seq_length = 512\ndo_train = True\ndo_eval = True\ndo_lower_case = True\ntrain_batch_size = 1\neval_batch_size = 1\nlearning_rate = 0.000001\nnum_train_epochs = 1\n#-------------------------------------------\nwarmup_proportion = 0.1\nno_cuda = False  # False = use cuda if available\nlocal_rank = -1\nseed = 42\n#------------------------------------------\ngradient_accumulation_steps = 2\nfp16 = False\nloss_scale = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenizer = RobertaTokenizer.from_pretrained(roberta_model, do_lower_case=do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"id":"1DVH8M2tzxgV","outputId":"8494f91e-4df7-4741-81d6-a445b0b0be4d","trusted":true},"cell_type":"code","source":"'''import zipfile\nimport os\nfor file_name in os.listdir(drive_dir):\n  if file_name.endswith('.zip'):\n    with zipfile.ZipFile(drive_dir+'/'+file_name,'r') as zip_dir:\n      zip_dir.extractall(path='/content/')'''","execution_count":null,"outputs":[]},{"metadata":{"id":"OtA_xDtWReUO","outputId":"cd85b46c-4cc9-4736-b281-1d6de09f9ba5","trusted":true},"cell_type":"code","source":"#import torch\nprint(torch.cuda.get_device_name(0))","execution_count":null,"outputs":[]},{"metadata":{"id":"AlcKyn1TwBQI","trusted":true},"cell_type":"code","source":"logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.INFO)\nlogger = logging.getLogger(__name__)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"OCqskyLcwElO","trusted":true},"cell_type":"code","source":"class RaceExample(object):\n    \"\"\"A single training/test example for the RACE dataset.\"\"\"\n    '''\n    For RACE dataset:\n    race_id: data id\n    context_sentence: article\n    start_ending: question\n    ending_0/1/2/3: option_0/1/2/3\n    label: true answer\n    '''\n    def __init__(self,\n                 race_id,\n                 context_sentence,\n                 start_ending,\n                 ending_0,\n                 ending_1,\n                 ending_2,\n                 ending_3,\n                 label = None):\n        self.race_id = race_id\n        self.context_sentence = context_sentence\n        self.start_ending = start_ending\n        self.endings = [\n            ending_0,\n            ending_1,\n            ending_2,\n            ending_3,\n        ]\n        self.label = label\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        l = [\n            f\"id: {self.race_id}\",\n            f\"article: {self.context_sentence}\",\n            f\"question: {self.start_ending}\",\n            f\"option_0: {self.endings[0]}\",\n            f\"option_1: {self.endings[1]}\",\n            f\"option_2: {self.endings[2]}\",\n            f\"option_3: {self.endings[3]}\",\n        ]\n\n        if self.label is not None:\n            l.append(f\"label: {self.label}\")\n\n        return \", \".join(l)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"anP7lPY6xkp_","trusted":true},"cell_type":"code","source":"class InputFeatures(object):\n    def __init__(self,\n                 example_id,\n                 choices_features,\n                 label\n\n    ):\n        self.example_id = example_id\n        self.choices_features = [\n            {\n                'input_ids': input_ids,\n                'input_mask': input_mask,\n                'segment_ids': segment_ids\n            }\n            for _, input_ids, input_mask, segment_ids in choices_features\n        ]\n        self.label = label","execution_count":null,"outputs":[]},{"metadata":{"id":"NpmlldL_xnh1","trusted":true},"cell_type":"code","source":"## paths is a list containing all paths\ndef read_race_examples(paths):\n    examples = []\n    for path in paths:\n        filenames = glob.glob(path+\"/*json\")\n        for filename in filenames:\n            with open(filename, 'r', encoding='utf-8') as fpr:\n                data_raw = json.load(fpr)\n                article = data_raw['article']\n                ## for each qn\n                for i in range(len(data_raw['answers'])):\n                    truth = ord(data_raw['answers'][i]) - ord('A')\n                    question = data_raw['questions'][i]\n                    options = data_raw['options'][i]\n                    examples.append(\n                        RaceExample(\n                            race_id = filename+'-'+str(i),\n                            context_sentence = article,\n                            start_ending = question,\n\n                            ending_0 = options[0],\n                            ending_1 = options[1],\n                            ending_2 = options[2],\n                            ending_3 = options[3],\n                            label = truth))\n                \n    return examples","execution_count":null,"outputs":[]},{"metadata":{"id":"vBWHk7eBxsog","trusted":true},"cell_type":"code","source":"def convert_examples_to_features(examples, tokenizer, max_seq_length,\n                                 is_training):\n    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n\n    # RACE is a multiple choice task. To perform this task using Bert,\n    # we will use the formatting proposed in \"Improving Language\n    # Understanding by Generative Pre-Training\" and suggested by\n    # @jacobdevlin-google in this issue\n    # https://github.com/google-research/bert/issues/38.\n    #\n    # The input will be like:\n    # [CLS] Article [SEP] Question + Option [SEP]\n    # for each option \n    # \n    # The model will output a single value for each input. To get the\n    # final decision of the model, we will run a softmax over these 4\n    # outputs.\n    features = []\n    for example_index, example in enumerate(examples):\n        context_tokens = tokenizer.tokenize(example.context_sentence)\n        start_ending_tokens = tokenizer.tokenize(example.start_ending)\n\n        choices_features = []\n        for ending_index, ending in enumerate(example.endings):\n            # We create a copy of the context tokens in order to be\n            # able to shrink it according to ending_tokens\n            context_tokens_choice = context_tokens[:]\n            ending_tokens = start_ending_tokens + tokenizer.tokenize(ending)\n            # Modifies `context_tokens_choice` and `ending_tokens` in\n            # place so that the total length is less than the\n            # specified length.  Account for [CLS], [SEP], [SEP] with\n            # \"- 3\"\n            _truncate_seq_pair(context_tokens_choice, ending_tokens, max_seq_length - 3)\n\n            tokens = [tokenizer.cls_token] + context_tokens_choice + [tokenizer.sep_token] + ending_tokens + [tokenizer.sep_token]\n            segment_ids = [0] * (len(context_tokens_choice) + 2) + [1] * (len(ending_tokens) + 1)\n\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n            input_mask = [1] * len(input_ids)\n\n            # Zero-pad up to the sequence length.\n            padding = [0] * (max_seq_length - len(input_ids))\n            input_ids += padding\n            input_mask += padding\n            segment_ids += padding\n\n            assert len(input_ids) == max_seq_length\n            assert len(input_mask) == max_seq_length\n            assert len(segment_ids) == max_seq_length\n\n            choices_features.append((tokens, input_ids, input_mask, segment_ids))\n\n        label = example.label\n        ## display some example\n        if example_index < 1:\n            logger.info(\"*** Example ***\")\n            logger.info(f\"race_id: {example.race_id}\")\n            for choice_idx, (tokens, input_ids, input_mask, segment_ids) in enumerate(choices_features):\n                logger.info(f\"choice: {choice_idx}\")\n                logger.info(f\"tokens: {' '.join(tokens)}\")\n                logger.info(f\"input_ids: {' '.join(map(str, input_ids))}\")\n                logger.info(f\"input_mask: {' '.join(map(str, input_mask))}\")\n                logger.info(f\"segment_ids: {' '.join(map(str, segment_ids))}\")\n            if is_training:\n                logger.info(f\"label: {label}\")\n\n        features.append(\n            InputFeatures(\n                example_id = example.race_id,\n                choices_features = choices_features,\n                label = label\n            )\n        )\n\n    return features","execution_count":null,"outputs":[]},{"metadata":{"id":"jwcBWSyAxulF","trusted":true},"cell_type":"code","source":"def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()","execution_count":null,"outputs":[]},{"metadata":{"id":"T16BaD2Yxwge","trusted":true},"cell_type":"code","source":"def accuracy(out, labels):\n    outputs = np.argmax(out, axis=1)\n    return np.sum(outputs == labels)","execution_count":null,"outputs":[]},{"metadata":{"id":"XB_sayHExyF1","trusted":true},"cell_type":"code","source":"def select_field(features, field):\n    return [\n        [\n            choice[field]\n            for choice in feature.choices_features\n        ]\n        for feature in features\n    ]","execution_count":null,"outputs":[]},{"metadata":{"id":"t4Sr4oXOxzrm","trusted":true},"cell_type":"code","source":"def warmup_linear(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 1.0 - x","execution_count":null,"outputs":[]},{"metadata":{"id":"_9jTTz1zx12f","trusted":true},"cell_type":"code","source":"def main():\n    global train_batch_size\n\n    if local_rank == -1 or no_cuda:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n        n_gpu = torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(local_rank)\n        device = torch.device(\"cuda\", local_rank)\n        n_gpu = 1\n        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.distributed.init_process_group(backend='nccl')\n    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n        device, n_gpu, bool(local_rank != -1), fp16))\n\n    if gradient_accumulation_steps < 1:\n        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n                            gradient_accumulation_steps))\n\n    train_batch_size = int(train_batch_size / gradient_accumulation_steps)\n    if train_batch_size == 0:\n        train_batch_size = 1\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if n_gpu > 0:\n        torch.cuda.manual_seed_all(seed)\n\n    if not do_train and not do_eval:\n        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n\n    '''if os.path.exists(output_dir) and os.listdir(output_dir):\n        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(output_dir))'''\n    os.makedirs(output_dir, exist_ok=True)\n\n    #tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n    tokenizer = RobertaTokenizer.from_pretrained(roberta_model, do_lower_case=do_lower_case)\n\n    train_examples = None\n    num_train_steps = None\n    if do_train:\n        train_dir = os.path.join(data_dir, 'train')\n        eval_dir = os.path.join(data_dir, 'dev')\n        train_examples = read_race_examples([train_dir,eval_dir])\n        #print(train_batch_size,gradient_accumulation_steps,num_train_epochs)\n        num_train_steps = int(\n            len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)\n\n    # Prepare model\n    model_state_dict = torch.load(basePath+'epoch3_Robert_base.bin')\n    model = RobertaForMultipleChoice.from_pretrained(roberta_model,state_dict=model_state_dict)\n    \n    #model = RobertaForMultipleChoice.from_pretrained(roberta_model)\n    \n\n    if fp16:\n        model.half()\n    model.to(device)\n    if local_rank != -1:\n        try:\n            from apex.parallel import DistributedDataParallel as DDP\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n\n        model = DDP(model)\n    elif n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Prepare optimizer\n    param_optimizer = list(model.named_parameters())\n\n    # hack to remove pooler, which is not used\n    # thus it produce None grad that break apex\n    param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n    t_total = num_train_steps\n    if local_rank != -1:\n        t_total = t_total // torch.distributed.get_world_size()\n    if fp16:\n        try:\n            #from apex.optimizers import FP16_Optimizer\n            from apex.fp16_utils.fp16_optimizer import FP16_Optimizer\n            from apex.optimizers import FusedAdam\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n\n        optimizer = FusedAdam(optimizer_grouped_parameters,\n                              lr=learning_rate,\n                              bias_correction=False)\n        if loss_scale == 0:\n            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n        else:\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale=loss_scale)\n    else:\n        optimizer = BertAdam(optimizer_grouped_parameters,\n                             lr=learning_rate,\n                             warmup=warmup_proportion,\n                             t_total=t_total)\n\n    global_step = 0\n    if do_train:\n        train_features = convert_examples_to_features(\n            train_examples, tokenizer, max_seq_length, True)\n        logger.info(\"***** Running training *****\")\n        logger.info(\"  Num examples = %d\", len(train_examples))\n        logger.info(\"  Batch size = %d\", train_batch_size)\n        logger.info(\"  Num steps = %d\", num_train_steps)\n        print('Num steps: ',num_train_steps)\n        all_input_ids = torch.tensor(select_field(train_features, 'input_ids'), dtype=torch.long)\n        all_input_mask = torch.tensor(select_field(train_features, 'input_mask'), dtype=torch.long)\n        all_segment_ids = torch.tensor(select_field(train_features, 'segment_ids'), dtype=torch.long)\n        all_label = torch.tensor([f.label for f in train_features], dtype=torch.long)\n        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n        if local_rank == -1:\n            train_sampler = RandomSampler(train_data)\n        else:\n            train_sampler = DistributedSampler(train_data)\n        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n\n        model.train()\n        for ep in range(int(num_train_epochs)):\n            tr_loss = 0\n            nb_tr_examples, nb_tr_steps = 0, 0\n            logger.info(\"Trianing Epoch: {}/{}\".format(ep+1, int(num_train_epochs)))\n            for step, batch in enumerate(train_dataloader):\n                batch = tuple(t.to(device) for t in batch)\n                input_ids, input_mask, segment_ids, label_ids = batch\n                loss,_ = model(input_ids = input_ids, attention_mask = input_mask,labels = label_ids)\n                if n_gpu > 1:\n                    loss = loss.mean() # mean() to average on multi-gpu.\n                if fp16 and loss_scale != 1.0:\n                    # rescale loss for fp16 training\n                    # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n                    loss = loss * loss_scale\n                if gradient_accumulation_steps > 1:\n                    loss = loss / gradient_accumulation_steps\n                tr_loss += loss.item()\n                nb_tr_examples += input_ids.size(0)\n                nb_tr_steps += 1\n\n                if fp16:\n                    optimizer.backward(loss)\n                else:\n                    loss.backward()\n                if (step + 1) % gradient_accumulation_steps == 0:\n                    # modify learning rate with special warm up BERT uses\n                    lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = lr_this_step\n                    optimizer.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n\n                if global_step%100 == 0:\n                    logger.info(\"Training loss: {}, global step: {}\".format(tr_loss/nb_tr_steps, global_step))\n                    print('loss: ',tr_loss/nb_tr_steps)\n                    print('global step: ',global_step)\n\n\n\n    # Save a trained model\n    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n\n    output_model_file = os.path.join(output_dir, \"pytorch_model_Robert.bin\")\n    torch.save(model_to_save.state_dict(), output_model_file)\n\n    '''output_model_file = os.path.join(drive_dir, \"pytorch_model.bin\")\n    torch.save(model_to_save.state_dict(), output_model_file)'''\n    \n\n\n    ## Load a trained model that you have fine-tuned\n    ## use this part if you want to load the trained model\n    '''model_state_dict = torch.load(basePath+'epoch3_Robert_base.bin')\n    model = RobertaForMultipleChoice.from_pretrained(roberta_model,state_dict=model_state_dict)\n    model.to(device)\n    print(\"model loaded...\")'''\n\n    if do_eval and (local_rank == -1 or torch.distributed.get_rank() == 0):\n        test_dir = os.path.join(data_dir, 'test')\n        test_high = [test_dir ]\n        #test_middle = [test_dir + '/middle']\n\n        ## test high \n        eval_examples = read_race_examples(test_high)\n        eval_features = convert_examples_to_features(\n            eval_examples, tokenizer, max_seq_length, True)\n        logger.info(\"***** Running evaluation: test high *****\")\n        print(\"***** Running evaluation: test high *****\")\n        logger.info(\"  Num examples = %d\", len(eval_examples))\n        print('Num examples: ',len(eval_examples))\n        logger.info(\"  Batch size = %d\", eval_batch_size)\n        all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long)\n        all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long)\n        all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long)\n        all_label = torch.tensor([f.label for f in eval_features], dtype=torch.long)\n        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n        # Run prediction for full data\n        eval_sampler = SequentialSampler(eval_data)\n        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n\n        model.eval()\n        high_eval_loss, high_eval_accuracy = 0, 0\n        high_nb_eval_steps, high_nb_eval_examples = 0, 0\n        for step, batch in enumerate(eval_dataloader):\n            batch = tuple(t.to(device) for t in batch)\n            input_ids, input_mask, segment_ids, label_ids = batch\n\n            with torch.no_grad():\n                tmp_eval_loss, _ = model(input_ids = input_ids, attention_mask = input_mask,labels = label_ids)\n                logits = model(input_ids = input_ids,attention_mask =  input_mask)\n\n            logits = logits[0].detach().cpu().numpy()\n            label_ids = label_ids.to('cpu').numpy()\n            tmp_eval_accuracy = accuracy(logits, label_ids)\n\n            high_eval_loss += tmp_eval_loss.mean().item()\n            high_eval_accuracy += tmp_eval_accuracy\n\n            high_nb_eval_examples += input_ids.size(0)\n            high_nb_eval_steps += 1\n            #return\n\n        eval_loss = high_eval_loss / high_nb_eval_steps\n        eval_accuracy = high_eval_accuracy / high_nb_eval_examples\n\n        result = {'high_eval_loss': eval_loss,\n                  'high_eval_accuracy': eval_accuracy}\n\n        output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n        with open(output_eval_file, \"a+\") as writer:\n            logger.info(\"***** Eval results *****\")\n            for key in sorted(result.keys()):\n                logger.info(\"  %s = %s\", key, str(result[key]))\n                print(key,' = ',str(result[key]))\n                writer.write(\"%s = %s\\n\" % (key, str(result[key])))        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"main()","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"BERT_MCQ.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}